### 6.2 图像卷积

特征映射（feature map）：输出的卷积层
感受野（receptive field）：指的是前向传播期间可能影响x计算的所有元素
感觉英文版的题目正常了许多。

### 6.3 填充和步幅

图片大小为$(n_h,n_w)$,核大小为$(k_h,k_w)$,padding大小为$(k_h,k_w)$，stride大小为$(s_h,s_w)$。

1. 没有padding的情况下，输出层大小为
   $$
        (n_h - k_h + 1) \times (n_w - k_w + 1)
   $$
2. 有padding的情况下，输出层大小为
   $$
        (n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)
   $$
3. 有padding+stride的情况下，输出层大小为
   $$
        (n_h - k_h + p_h + 1) / s_h \times (n_w - k_w + p_w + 1)/ s_w, if (n_h - k_h + p_h + 1) \mid s_h \wedge (n_h - k_h + p_h + 1) \mid s_w 
   $$
   $$
        \lfloor{(n_h - k_h + p_h + 1) / s_h}\rfloor \times \lfloor{(n_w - k_w + p_w + 1)/ s_w}\rfloor, if (n_h - k_h + p_h + 1) \nmid s_h \vee (n_h - k_h + p_h + 1) \nmid s_w
   $$

### 6.4 多输入多输出通道
主要讲了多输入和多输出层的卷积。
   

**6.2 习题**
1. 构建一个具有对角线边缘的图像X。
   1. 如果将本节中举例的卷积核K应用于X，会发生什么情况？
   ```python
    tensor([[ 1.,  0.,  0.,  0.,  0.],
            [-1.,  1.,  0.,  0.,  0.],
            [ 0., -1.,  1.,  0.,  0.],
            [ 0.,  0., -1.,  1.,  0.],
            [ 0.,  0.,  0., -1.,  1.],
            [ 0.,  0.,  0.,  0., -1.]])
    ```
   2. 如果转置X会发生什么？
   ```python
    tensor([[ 1.,  0.,  0.,  0.,  0.],
            [-1.,  1.,  0.,  0.,  0.],
            [ 0., -1.,  1.,  0.,  0.],
            [ 0.,  0., -1.,  1.,  0.],
            [ 0.,  0.,  0., -1.,  1.],
            [ 0.,  0.,  0.,  0., -1.]])
   ```
   3. 如果转置K会发生什么？
   ```python
    tensor([[ 1., -1.,  0.,  0.,  0.,  0.],
            [ 0.,  1., -1.,  0.,  0.,  0.],
            [ 0.,  0.,  1., -1.,  0.,  0.],
            [ 0.,  0.,  0.,  1., -1.,  0.],
            [ 0.,  0.,  0.,  0.,  1., -1.]])
   ```
2. 在我们创建的Conv2D自动求导时，有什么错误消息？
3. 如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？
   ```python
   def corr2d(X,K):
    h,w = K.shape
    outh,outw = X.shape[0]-h+1,X.shape[1]-w+1
    XX = torch.zeros(k,outw*outh)
    for i in range(h):
        for j in range(w):
            XX[:,k] = X[i:i+h,j:j+w].reshape(-1)
            k+=1
    res = (torch.mm(k,XX)).reshape(outh,outw)
    return res 
   ```
4. 手工设计一些卷积核。
   1. 二阶导数的核的形式是什么？
   网站底下的同学提到是拉普拉斯算子。
   2. 积分的核的形式是什么？
   这里询问了万能的gpt。感觉回答的怪怪的，好似在回答常微分方程的事情。回答的是当求解非齐次方程时，可以求解齐次方程的解。
   $$
   y(x) = y_h(x) + \int K(x,t)f(t)dt
   $$
   K(x,t)为积分核，f(t)为已知函数。
   3. 得到d次导数的最小核的大小是多少？

**6.4习题**

1. 太简单了（不过看到有的同学在这里还引用了论文，蚌埠住了。
   问的是卷积符合结合律吗，然后引用的论文是[Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)
2. 输入为$c_i \times h \times w$，卷积核大小为$c_o \times c_i \times k_h \times k_w$，填充为$(p_h,p_w)$，步幅为$(s_h,s_w)$
   1. 前向传播的计算成本
   > 乘法成本： 
   > $$c_o \times c_i \times (k_h*k_w) \times {\lfloor{(h - k_h + p_h + 1) / s_h}\rfloor \times \lfloor{(w - k_w + p_w + 1)/ s_w}\rfloor}$$
   > 加法成本：假设kernel_size = n,需要执行n-1加法。
   > $$
   c_o \times c_i \times (k_h*k_w - 1) \times {\lfloor{(h - k_h + p_h + 1) / s_h}\rfloor \times \lfloor{(w - k_w + p_w + 1)/ s_w}\rfloor}
   $$
   2. 内存占用
   > 包括输入，输出，以及卷积核大小。
   > 输入大小为
   > $$ c_i * h * w $$
   > 输出大小为
   > $$c_o * c_j * {\lfloor{(h - k_h + p_h + 1) / s_h}\rfloor \times \lfloor{(w - k_w + p_w + 1)/ s_w}\rfloor}$$
   > 卷积核大小为
   > $c_o \times c_j \times k_h \times k_w$ 
   3. 反向传播的内存占用
   4. 反向传播的计算成本
   > 与前向传播相同，为了计算损失函数关于卷积核权重的偏导数

   > 关于乘法的反向传播计算成本为
   > $$c_o \times c_j \times (k_h*k_w) {\lfloor{(h - k_h + p_h + 1) / s_h}\rfloor \times \lfloor{(w - k_w + p_w + 1)/ s_w}\rfloor}$$
   > 关于加法的反向传播计算成本为
   > $$ c_o * c_j * {\lfloor{(h - k_h + p_h + 1) / s_h}\rfloor \times \lfloor{(w - k_w + p_w + 1)/ s_w}\rfloor} $$
   